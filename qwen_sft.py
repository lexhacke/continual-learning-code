import copy, torch, os
import torch.nn as nn
import torch.nn.functional as F
from lightning.pytorch.loggers import TensorBoardLogger
import lightning.pytorch as L
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from typing import Dict, List
from pyngrok import ngrok

class SDFTModule(L.LightningModule):
    """
    Self-Distillation Fine-Tuning (SDFT) for Continual Learning.

    From "Self-Distillation Enables Continual Learning" (Shenfeld et al., 2026):
    - Teacher: model conditioned on demonstration (context + prompt)
    - Student: model conditioned only on prompt
    - Loss: Reverse KL divergence DKL(student || teacher)
    - On-policy: samples generated by student, trained to match teacher

    The key insight is that in-context learning creates a teacher that is
    close to the base model while achieving optimal task performance.
    """

    def __init__(
        self,
        model_name: str = "Qwen/Qwen2.5-Coder-0.5B-Instruct",
        learning_rate: float = 1e-5,
        ema_decay: float = 0.99,
        max_new_tokens: int = 512,
        temperature: float = 1.0,
    ):
        super().__init__()
        self.save_hyperparameters()

        # Student model (trainable) - loaded in FP8 for memory efficiency
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_enable_fp32_cpu_offload=False,
        )
        self.student = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2",
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # Teacher model (EMA of student, frozen)
        self.teacher = copy.deepcopy(self.student)
        for param in self.teacher.parameters():
            param.requires_grad = False

        self.ema_decay = ema_decay
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature

    def _build_teacher_prompt(self, context: str, prompt: str) -> str:
        """
        Build the teacher prompt with demonstration context.
        Following the paper's prompt template for in-context learning.
        """
        return f"""<Question>
{prompt}

This is the surrounding code context for reference:
<Context>
{context}

Now generate the code that satisfies the specification above:"""

    def _build_student_prompt(self, prompt: str) -> str:
        """
        Build the student prompt without demonstration context.
        Student only sees the specification, not the surrounding code.
        """
        return f"""<Question>
{prompt}

Now generate the code that satisfies the specification above:"""

    @torch.no_grad()
    def _update_ema(self):
        """Update teacher with exponential moving average of student weights."""
        for teacher_param, student_param in zip(
            self.teacher.parameters(), self.student.parameters()
        ):
            teacher_param.data.mul_(self.ema_decay).add_(
                student_param.data, alpha=1 - self.ema_decay
            )

    @torch.no_grad()
    def _generate_on_policy_samples(
        self, student_inputs: Dict[str, torch.Tensor]
    ) -> torch.Tensor:
        """
        Generate on-policy samples from the student model.
        This is critical for SDFT - we train on student's own distribution.
        """
        self.student.eval()
        outputs = self.student.generate(
            input_ids=student_inputs["input_ids"],
            attention_mask=student_inputs["attention_mask"],
            max_new_tokens=self.max_new_tokens,
            do_sample=True,
            temperature=self.temperature,
            pad_token_id=self.tokenizer.pad_token_id,
            return_dict_in_generate=True,
        )
        self.student.train()
        return outputs.sequences

    def _compute_generated_log_probs(
        self,
        model: nn.Module,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        prompt_lengths: List[int],
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Compute per-token log probabilities for ONLY the generated portion.
        Returns (log_probs, mask) where both have shape [batch, max_gen_len].

        This ensures teacher and student log probs are aligned despite
        having different prompt lengths.
        """
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
        )
        logits = outputs.logits  # [batch, seq_len, vocab]

        batch_size = input_ids.shape[0]
        gen_lengths = [
            int(attention_mask[i].sum().item()) - prompt_lengths[i]
            for i in range(batch_size)
        ]
        max_gen_len = max(gen_lengths)

        all_log_probs = []
        all_masks = []

        for i in range(batch_size):
            plen = prompt_lengths[i]
            seq_len = int(attention_mask[i].sum().item())
            gen_len = seq_len - plen

            if gen_len > 0:
                # Logits at position plen-1 predict token at plen, etc.
                gen_logits = logits[i, plen-1:seq_len-1, :]  # [gen_len, vocab]
                gen_targets = input_ids[i, plen:seq_len]  # [gen_len]

                log_probs = F.log_softmax(gen_logits / self.temperature, dim=-1)
                token_log_probs = torch.gather(
                    log_probs, dim=-1, index=gen_targets.unsqueeze(-1)
                ).squeeze(-1)

                # Pad to max_gen_len
                pad_len = max_gen_len - gen_len
                padded_lp = F.pad(token_log_probs, (0, pad_len), value=0.0)
                mask = F.pad(torch.ones(gen_len, device=self.device), (0, pad_len), value=0.0)
            else:
                padded_lp = torch.zeros(max_gen_len, device=self.device)
                mask = torch.zeros(max_gen_len, device=self.device)

            all_log_probs.append(padded_lp)
            all_masks.append(mask)

        return torch.stack(all_log_probs), torch.stack(all_masks)

    def _concat_prompt_and_generated(
        self,
        prompt_ids: torch.Tensor,
        prompt_mask: torch.Tensor,
        generated_ids: torch.Tensor,
        max_length: int,
    ) -> tuple[torch.Tensor, torch.Tensor, List[int]]:
        """
        Concatenate prompt tokens with generated tokens directly (no decode/re-encode).
        Returns (input_ids, attention_mask, prompt_lengths).
        """
        batch_size = prompt_ids.shape[0]
        device = prompt_ids.device

        all_ids = []
        all_masks = []
        prompt_lengths = []

        for i in range(batch_size):
            # Get actual prompt length (excluding padding)
            plen = int(prompt_mask[i].sum().item())
            prompt = prompt_ids[i, :plen]

            # Get actual generated length (excluding padding)
            gen_mask = generated_ids[i] != self.tokenizer.pad_token_id
            gen_len = int(gen_mask.sum().item())
            gen = generated_ids[i, :gen_len]

            # Concatenate
            combined = torch.cat([prompt, gen])
            combined_mask = torch.ones(len(combined), device=device)

            all_ids.append(combined)
            all_masks.append(combined_mask)
            prompt_lengths.append(plen)

        # Pad to same length
        max_len = min(max(len(ids) for ids in all_ids), max_length)
        padded_ids = torch.full((batch_size, max_len), self.tokenizer.pad_token_id, device=device)
        padded_mask = torch.zeros((batch_size, max_len), device=device)

        for i, (ids, mask) in enumerate(zip(all_ids, all_masks)):
            length = min(len(ids), max_len)
            padded_ids[i, :length] = ids[:length]
            padded_mask[i, :length] = mask[:length]

        return padded_ids, padded_mask, prompt_lengths

    def training_step(self, batch: Dict[str, str], batch_idx: int) -> torch.Tensor:
        """
        SDFT Training Step:
        1. Build teacher input (context + prompt) and student input (prompt only)
        2. Generate on-policy samples from student
        3. Concatenate prompts + generated tokens DIRECTLY (no decode/re-encode)
        4. Compute log probs on identical generated tokens for both
        5. Minimize reverse KL divergence: DKL(student || teacher)
        """
        contexts = batch["context"]
        prompts = batch["prompt"]

        # Build prompts for teacher and student
        teacher_prompts = [
            self._build_teacher_prompt(ctx, pmt)
            for ctx, pmt in zip(contexts, prompts)
        ]
        student_prompts = [
            self._build_student_prompt(pmt)
            for pmt in prompts
        ]

        # Tokenize prompts separately
        student_prompt_tokens = self.tokenizer(
            student_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096,
        ).to(self.device)

        teacher_prompt_tokens = self.tokenizer(
            teacher_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=8192,
        ).to(self.device)

        # Generate on-policy samples from student (Equation 2: y ~ πθ)
        with torch.no_grad():
            generated_sequences = self._generate_on_policy_samples(student_prompt_tokens)

        # Extract ONLY the generated token IDs (no decode/re-encode)
        student_prompt_len = student_prompt_tokens["input_ids"].shape[1]
        generated_ids = generated_sequences[:, student_prompt_len:]

        # Concatenate prompts + generated tokens directly
        # This ensures EXACT same generated tokens for both teacher and student
        student_ids, student_mask, student_prompt_lens = self._concat_prompt_and_generated(
            student_prompt_tokens["input_ids"],
            student_prompt_tokens["attention_mask"],
            generated_ids,
            max_length=4096 + self.max_new_tokens,
        )

        teacher_ids, teacher_mask, teacher_prompt_lens = self._concat_prompt_and_generated(
            teacher_prompt_tokens["input_ids"],
            teacher_prompt_tokens["attention_mask"],
            generated_ids,
            max_length=8192 + self.max_new_tokens,
        )

        # Compute student log probs on generated tokens only
        student_log_probs, mask = self._compute_generated_log_probs(
            self.student,
            student_ids,
            student_mask,
            student_prompt_lens,
        )

        # Compute teacher log probs on SAME generated tokens (detached)
        with torch.no_grad():
            teacher_log_probs, _ = self._compute_generated_log_probs(
                self.teacher,
                teacher_ids,
                teacher_mask,
                teacher_prompt_lens,
            )

        # Reverse KL: DKL(student || teacher) = E[log student - log teacher]
        # From Equation 1: L(θ) = E[log πθ(y|x) - log π(y|x, c)]
        kl_per_token = (student_log_probs - teacher_log_probs) * mask
        loss = kl_per_token.sum() / mask.sum().clamp(min=1)

        # Update EMA teacher
        self._update_ema()

        # Logging
        self.log("train/loss", loss, prog_bar=True)

        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.student.parameters(),
            lr=self.hparams.learning_rate,
            weight_decay=0.01,
        )
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=self.trainer.estimated_stepping_batches
        )
        return {
            "optimizer": optimizer,
            "lr_scheduler": {"scheduler": scheduler, "interval": "step"},
        }


def train_sdft(
    dataset,
    launch_subprocess: bool = False,
    logger_dir: str = "./sdft_logs",
    model_name: str = "Qwen/Qwen2.5-Coder-7B-Instruct",
    batch_size: int = 4,
    max_epochs: int = 3,
    learning_rate: float = 1e-5,
    accumulate_grad_batches: int = 4,
    output_dir: str = "./sdft_checkpoints",
):
    """
    Train a model using SDFT for continual learning.

    Args:
        dataset: CodeFinetuneDataset returning {context, prompt, code}
        model_name: Qwen model to fine-tune
        batch_size: Training batch size
        max_epochs: Number of training epochs
        learning_rate: Learning rate for AdamW
        accumulate_grad_batches: Gradient accumulation steps
        output_dir: Directory for checkpoints
    """
    # Initialize model
    model = SDFTModule(
        model_name=model_name,
        learning_rate=learning_rate,
    )
    
    logger = TensorBoardLogger(logger_dir, name="sdft")

    if launch_subprocess:
        import subprocess
        tb_process = subprocess.Popen(['tensorboard', '--logdir', logger_dir, '--port', '6006'])
        ngrok.set_auth_token(os.environ['NGROK_KEY'])
        url = ngrok.connect(6006)
        print("Tensorboard URL:", url)


    # DataLoader with custom collate
    def collate_fn(batch):
        return {
            "context": [item["context"] for item in batch],
            "prompt": [item["prompt"] for item in batch],
            "code": [item["code"] for item in batch],
        }

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        collate_fn=collate_fn,
    )

    # Trainer
    trainer = L.Trainer(
        max_epochs=max_epochs,
        accelerator="auto",
        devices=1,
        precision="bf16-mixed",
        accumulate_grad_batches=accumulate_grad_batches,
        gradient_clip_val=1.0,
        default_root_dir=output_dir,
        log_every_n_steps=10,
        callbacks=[
            L.callbacks.ModelCheckpoint(
                save_top_k=3,
                monitor="train/loss",
                mode="min",
            ),
            L.callbacks.LearningRateMonitor(logging_interval="step"),
        ],
        logger=logger,
    )

    trainer.fit(model, dataloader)
    if launch_subprocess:
        tb_process.terminate()
    return model


if __name__ == "__main__":
    from code_dataset import CodeFinetuneDataset
    # Load dataset
    dataset = CodeFinetuneDataset("https://github.com/psf/requests")
    print(f"Loaded {len(dataset)} samples")

    # Train with SDFT
    model = train_sdft(
        dataset,
        model_name="Qwen/Qwen2.5-Coder-7B-Instruct",
        batch_size=2,
        max_epochs=3,
    )
