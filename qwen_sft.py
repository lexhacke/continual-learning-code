import torch, os
import torch.nn.functional as F
from lightning.pytorch.loggers import TensorBoardLogger
import lightning.pytorch as L
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
from typing import Dict, List
from pyngrok import ngrok

class SDFTModule(L.LightningModule):
    """
    Self-Distillation Fine-Tuning (SDFT) for Continual Learning with PEFT/LoRA.

    From "Self-Distillation Enables Continual Learning" (Shenfeld et al., 2026):
    - Teacher: model conditioned on demonstration (context + prompt)
    - Student: model conditioned only on prompt
    - Loss: Reverse KL divergence DKL(student || teacher)
    - On-policy: samples generated by student, trained to match teacher

    Optimizations:
    - Multi-adapter PEFT: student ("default") and teacher adapters share base model
    - Fused EMA via torch.lerp_ (single CUDA kernel)
    - Debiased EMA for stable early training
    """

    def __init__(
        self,
        model_name: str = "Qwen/Qwen2.5-Coder-0.5B-Instruct",
        learning_rate: float = 1e-4,
        ema_decay: float = 0.99,
        max_new_tokens: int = 512,
        temperature: float = 1.0,
        lora_r: int = 64,
        lora_alpha: int = 128,
        lora_dropout: float = 0.05,
        flash_attn: bool = False,
        use_debiased_ema: bool = True,
    ):
        super().__init__()
        self.save_hyperparameters()

        # Load base model in bf16
        base_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            attn_implementation="flash_attention_2" if flash_attn else 'sdpa',
        )

        # LoRA config targeting attention + MLP layers
        lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            lora_dropout=lora_dropout,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            bias="none",
            task_type="CAUSAL_LM",
        )

        # Student model with "default" adapter (trainable)
        self.student = get_peft_model(base_model, lora_config)
        self.student.print_trainable_parameters()

        # Add teacher adapter (shares base model, zero extra memory for base)
        self.student.add_adapter("teacher", lora_config)

        # Freeze teacher adapter weights
        self.student.set_adapter("teacher")
        for param in self.student.parameters():
            if param.requires_grad:
                param.requires_grad = False

        # Switch back to student (default) adapter
        self.student.set_adapter("default")

        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.tokenizer.padding_side = "left"
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.ema_decay = ema_decay
        self.max_new_tokens = max_new_tokens
        self.temperature = temperature
        self.use_debiased_ema = use_debiased_ema

        # EMA step counter for debiasing
        self.register_buffer("ema_step", torch.tensor(0, dtype=torch.long))

    def _get_adapter_params(self, adapter_name: str) -> Dict[str, torch.nn.Parameter]:
        """Get parameters for a specific adapter."""
        params = {}
        for name, param in self.student.named_parameters():
            if f".{adapter_name}." in name or name.endswith(f".{adapter_name}"):
                # Normalize name to match between adapters
                base_name = name.replace(f".{adapter_name}.", ".ADAPTER.").replace(f".{adapter_name}", ".ADAPTER")
                params[base_name] = param
        return params

    @torch.no_grad()
    def _update_ema(self):
        """Fused EMA update using torch.lerp_ with optional debiasing."""
        self.ema_step += 1

        student_params = self._get_adapter_params("default")
        teacher_params = self._get_adapter_params("teacher")

        # Compute effective weight for lerp (handles debiasing)
        if self.use_debiased_ema:
            # Debiased EMA: corrects for initialization bias in early steps
            bias_correction = 1.0 - (self.ema_decay ** self.ema_step.item())
            effective_weight = (1.0 - self.ema_decay) / bias_correction
            effective_weight = min(effective_weight, 1.0)  # Clamp for stability
        else:
            effective_weight = 1.0 - self.ema_decay

        # Fused update: teacher = lerp(teacher, student, weight)
        for name, student_param in student_params.items():
            if name in teacher_params:
                teacher_param = teacher_params[name]
                # Cast weight to match param dtype for lerp_
                weight = torch.tensor(effective_weight, dtype=teacher_param.dtype, device=teacher_param.device)
                teacher_param.data.lerp_(student_param.data, weight)

    def _build_teacher_prompt(self, context: str, prompt: str) -> str:
        return f"""<Question>
{prompt}

This is the surrounding code context for reference:
<Context>
{context}

Now generate the code that satisfies the specification above:"""

    def _build_student_prompt(self, prompt: str) -> str:
        return f"""<Question>
{prompt}

Now generate the code that satisfies the specification above:"""

    @torch.no_grad()
    def _generate_on_policy_samples(
        self, student_inputs: Dict[str, torch.Tensor]
    ) -> torch.Tensor:
        """Generate on-policy samples from the student model."""
        self.student.eval()
        self.student.set_adapter("default")
        outputs = self.student.generate(
            input_ids=student_inputs["input_ids"],
            attention_mask=student_inputs["attention_mask"],
            max_new_tokens=self.max_new_tokens,
            do_sample=True,
            temperature=self.temperature,
            pad_token_id=self.tokenizer.pad_token_id,
            return_dict_in_generate=True,
        )
        self.student.train()
        return outputs.sequences

    def _compute_generated_log_probs(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        prompt_lengths: List[int],
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """Compute per-token log probabilities for the generated portion."""
        outputs = self.student(
            input_ids=input_ids,
            attention_mask=attention_mask,
        )
        logits = outputs.logits

        batch_size = input_ids.shape[0]
        gen_lengths = [
            int(attention_mask[i].sum().item()) - prompt_lengths[i]
            for i in range(batch_size)
        ]
        max_gen_len = max(gen_lengths)

        all_log_probs = []
        all_masks = []

        for i in range(batch_size):
            plen = prompt_lengths[i]
            seq_len = int(attention_mask[i].sum().item())
            gen_len = seq_len - plen

            if gen_len > 0:
                gen_logits = logits[i, plen-1:seq_len-1, :]
                gen_targets = input_ids[i, plen:seq_len]

                log_probs = F.log_softmax(gen_logits / self.temperature, dim=-1)
                token_log_probs = torch.gather(
                    log_probs, dim=-1, index=gen_targets.unsqueeze(-1)
                ).squeeze(-1)

                pad_len = max_gen_len - gen_len
                padded_lp = F.pad(token_log_probs, (0, pad_len), value=0.0)
                mask = F.pad(torch.ones(gen_len, device=self.device), (0, pad_len), value=0.0)
            else:
                padded_lp = torch.zeros(max_gen_len, device=self.device)
                mask = torch.zeros(max_gen_len, device=self.device)

            all_log_probs.append(padded_lp)
            all_masks.append(mask)

        return torch.stack(all_log_probs), torch.stack(all_masks)

    def _concat_prompt_and_generated(
        self,
        prompt_ids: torch.Tensor,
        prompt_mask: torch.Tensor,
        generated_ids: torch.Tensor,
        max_length: int,
    ) -> tuple[torch.Tensor, torch.Tensor, List[int]]:
        """Concatenate prompt tokens with generated tokens."""
        batch_size = prompt_ids.shape[0]
        device = prompt_ids.device

        all_ids = []
        all_masks = []
        prompt_lengths = []

        for i in range(batch_size):
            plen = int(prompt_mask[i].sum().item())
            prompt = prompt_ids[i, :plen]

            gen_mask = generated_ids[i] != self.tokenizer.pad_token_id
            gen_len = int(gen_mask.sum().item())
            gen = generated_ids[i, :gen_len]

            combined = torch.cat([prompt, gen])
            combined_mask = torch.ones(len(combined), device=device)

            all_ids.append(combined)
            all_masks.append(combined_mask)
            prompt_lengths.append(plen)

        max_len = min(max(len(ids) for ids in all_ids), max_length)
        padded_ids = torch.full((batch_size, max_len), self.tokenizer.pad_token_id, device=device)
        padded_mask = torch.zeros((batch_size, max_len), device=device)

        for i, (ids, mask) in enumerate(zip(all_ids, all_masks)):
            length = min(len(ids), max_len)
            padded_ids[i, :length] = ids[:length]
            padded_mask[i, :length] = mask[:length]

        return padded_ids, padded_mask, prompt_lengths

    def training_step(self, batch: Dict[str, str], batch_idx: int) -> torch.Tensor:
        """SDFT Training Step with optimized multi-adapter PEFT."""
        contexts = batch["context"]
        prompts = batch["prompt"]

        teacher_prompts = [
            self._build_teacher_prompt(ctx, pmt)
            for ctx, pmt in zip(contexts, prompts)
        ]
        student_prompts = [
            self._build_student_prompt(pmt)
            for pmt in prompts
        ]

        student_prompt_tokens = self.tokenizer(
            student_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096,
        ).to(self.device)

        teacher_prompt_tokens = self.tokenizer(
            teacher_prompts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=8192,
        ).to(self.device)

        # Generate on-policy samples from student
        with torch.no_grad():
            generated_sequences = self._generate_on_policy_samples(student_prompt_tokens)

        student_prompt_len = student_prompt_tokens["input_ids"].shape[1]
        generated_ids = generated_sequences[:, student_prompt_len:]

        student_ids, student_mask, student_prompt_lens = self._concat_prompt_and_generated(
            student_prompt_tokens["input_ids"],
            student_prompt_tokens["attention_mask"],
            generated_ids,
            max_length=4096 + self.max_new_tokens,
        )

        teacher_ids, teacher_mask, teacher_prompt_lens = self._concat_prompt_and_generated(
            teacher_prompt_tokens["input_ids"],
            teacher_prompt_tokens["attention_mask"],
            generated_ids,
            max_length=8192 + self.max_new_tokens,
        )

        # Compute student log probs (default adapter)
        self.student.set_adapter("default")
        student_log_probs, mask = self._compute_generated_log_probs(
            student_ids,
            student_mask,
            student_prompt_lens,
        )

        # Compute teacher log probs (teacher adapter) - no cloning needed!
        with torch.no_grad():
            self.student.set_adapter("teacher")
            teacher_log_probs, _ = self._compute_generated_log_probs(
                teacher_ids,
                teacher_mask,
                teacher_prompt_lens,
            )
            self.student.set_adapter("default")

        # Reverse KL divergence
        kl_per_token = (student_log_probs - teacher_log_probs) * mask
        loss = kl_per_token.sum() / mask.sum().clamp(min=1)

        # Fused EMA update with debiasing
        self._update_ema()

        self.log("train/loss", loss, prog_bar=True)
        self.log("ema_step", self.ema_step.float(), prog_bar=False)
        return loss

    def configure_optimizers(self):
        # Only optimize student (default) adapter parameters
        self.student.set_adapter("default")
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.student.parameters()),
            lr=self.hparams.learning_rate,
            weight_decay=0.01,
        )
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=self.trainer.estimated_stepping_batches
        )
        return {
            "optimizer": optimizer,
            "lr_scheduler": {"scheduler": scheduler, "interval": "step"},
        }


def train_sdft(
    dataset,
    launch_subprocess: bool = False,
    logger_dir: str = "./sdft_logs",
    model_name: str = "Qwen/Qwen2.5-Coder-7B-Instruct",
    batch_size: int = 4,
    max_epochs: int = 3,
    learning_rate: float = 1e-4,
    accumulate_grad_batches: int = 4,
    output_dir: str = "./sdft_checkpoints",
    lora_r: int = 64,
    lora_alpha: int = 128,
):
    """Train a model using SDFT with PEFT/LoRA."""
    model = SDFTModule(
        model_name=model_name,
        learning_rate=learning_rate,
        lora_r=lora_r,
        lora_alpha=lora_alpha,
    )

    logger = TensorBoardLogger(logger_dir, name="sdft")

    if launch_subprocess:
        import subprocess
        tb_process = subprocess.Popen(['tensorboard', '--logdir', logger_dir, '--port', '6006'])
        ngrok.set_auth_token(os.environ['NGROK_KEY'])
        url = ngrok.connect(6006)
        print("Tensorboard URL:", url)

    def collate_fn(batch):
        return {
            "context": [item["context"] for item in batch],
            "prompt": [item["prompt"] for item in batch],
            "code": [item["code"] for item in batch],
        }

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        collate_fn=collate_fn,
    )

    trainer = L.Trainer(
        max_epochs=max_epochs,
        accelerator="auto",
        devices=1,
        precision="bf16-mixed",
        accumulate_grad_batches=accumulate_grad_batches,
        gradient_clip_val=1.0,
        default_root_dir=output_dir,
        log_every_n_steps=10,
        callbacks=[
            L.callbacks.ModelCheckpoint(
                save_top_k=3,
                monitor="train/loss",
                mode="min",
            ),
            L.callbacks.LearningRateMonitor(logging_interval="step"),
        ],
        logger=logger,
    )

    trainer.fit(model, dataloader)
    if launch_subprocess:
        tb_process.terminate()
    return model


if __name__ == "__main__":
    from code_dataset import CodeFinetuneDataset
    dataset = CodeFinetuneDataset("https://github.com/psf/requests")
    print(f"Loaded {len(dataset)} samples")

    model = train_sdft(
        dataset,
        model_name="Qwen/Qwen2.5-Coder-0.5B-Instruct",
        batch_size=2,
        max_epochs=3,
    )
